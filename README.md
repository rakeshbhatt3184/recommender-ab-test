# A/B Testing for a Content Recommendation Model 

This project demonstrates a complete MLOps pipeline for developing, deploying, and analyzing a content-based article recommendation system. It features a live A/B test between two different recommendation models served via a containerized REST API.

The primary goal is to showcase the full lifecycle of a machine learning model, from offline training and evaluation to live performance monitoring and data-driven decision-making in a production-like environment.



##  Key MLOps Features

* **Model Versioning & A/B Testing:** Deploys two distinct models (a classic TF-IDF approach and a modern Sentence-Transformer) under a single API endpoint to compare their real-world performance (latency).
* **API Deployment:** The models are served via a high-performance REST API built with **FastAPI**.
* **Containerization:** The entire application, including dependencies and model artifacts, is packaged into a **Docker** container for consistent and reproducible deployments.
* **Performance Monitoring:** The API logs key performance metrics (latency) for every request, enabling offline analysis to determine the winning model.

##  Tech Stack

* **Python 3.10**
* **ML & Data Science:** Pandas, Scikit-learn, Sentence-Transformers
* **API & Server:** FastAPI, Uvicorn
* **Containerization:** Docker
* **Analysis & Visualization:** Matplotlib

##  How to Run Locally

#### 1. Clone the Repository
```bash
git clone [https://github.com/](https://github.com/)<your-github-username>/recommender-ab-test.git
cd recommender-ab-test
```

#### 2. Set Up Environment
Create and activate a Python virtual environment.
```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

#### 3. Download Data
Download the [All the News dataset from Kaggle](https://www.kaggle.com/datasets/snapcrack/all-the-news). Place the `articles1.csv` file in the root of the project directory.

#### 4. Prepare Data & Train Models
Run the offline scripts to process the data and build the model similarity matrices.
```bash
python prepare_data.py
python train_models.py
```

#### 5. Build and Run the Docker Container
Ensure Docker Desktop is running.
```bash
# Build the Docker image
docker build -t recommender-ab-test .

# Run the container
docker run -p 8000:80 -d recommender-ab-test
```

#### 6. Test the API
The API is now running. Access the interactive documentation at **[http://localhost:8000/docs](http://localhost:8000/docs)** to send requests.

##  Project Structure
```
.
├── Dockerfile              # Instructions for building the Docker container
├── README.md               # This README file
├── ab_test_results.png     # Example output chart from the analysis
├── analyze_results.py      # Script to analyze log.csv and determine the winner
├── articles1.csv           # Raw data (needs to be downloaded)
├── log.csv                 # Log file generated by the API (created after running)
├── main.py                 # FastAPI application with A/B testing logic
├── model_A_similarity.pkl  # Saved model artifact for TF-IDF
├── model_B_similarity.pkl  # Saved model artifact for Sentence-Transformer
├── model_data.csv          # Processed data used by the models
├── prepare_data.py         # Script to clean and sample the raw data
├── requirements.txt        # Python package dependencies
└── train_models.py         # Script for offline model training
```

##  Analysis & Results

After generating traffic to the API, the `log.csv` file can be analyzed to compare model performance.

1.  **Stop the container and copy the logs:**
    ```bash
    docker ps
    docker stop <container_id>
    docker cp <container_id>:/app/log.csv .
    ```
2.  **Run the analysis script:**
    ```bash
    python analyze_results.py
    ```

This script prints the average latency for each model and generates a bar chart visualizing the results, allowing a data-driven decision on which model to fully deploy.